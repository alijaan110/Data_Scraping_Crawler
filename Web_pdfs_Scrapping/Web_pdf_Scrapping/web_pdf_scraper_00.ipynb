{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IJJtyg_uKeN",
        "outputId": "502c56dc-f804-4600-8a8d-66d82ba5933c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.parse\n",
        "import os\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "USER_AGENTS = [\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15',\n",
        "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'\n",
        "]\n",
        "\n",
        "def search_google_scholar(query, num_results=10):\n",
        "    base_url = \"https://scholar.google.com/scholar\"\n",
        "    params = {\n",
        "        'q': query,\n",
        "        'hl': 'en',\n",
        "        'as_sdt': '0,5',  # This parameter includes \"All\" and \"Reviews\" in the search\n",
        "        'as_vis': '1',    # Include citations\n",
        "        'num': num_results\n",
        "    }\n",
        "    headers = {'User-Agent': random.choice(USER_AGENTS)}\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        results = []\n",
        "        for div in soup.find_all('div', class_='gs_r gs_or gs_scl'):\n",
        "            for link in div.find_all('a'):\n",
        "                href = link.get('href')\n",
        "                if href and (href.lower().endswith('.pdf') or '[PDF]' in link.text):\n",
        "                    results.append(href)\n",
        "                    break  # Only take the first PDF link from each result\n",
        "            if len(results) >= num_results:\n",
        "                break\n",
        "\n",
        "        logging.info(f\"Found {len(results)} PDF results for query: {query}\")\n",
        "        return results\n",
        "    except requests.RequestException as e:\n",
        "        logging.error(f\"Error during search: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def download_pdf(url, folder):\n",
        "    try:\n",
        "        headers = {'User-Agent': random.choice(USER_AGENTS)}\n",
        "        response = requests.get(url, headers=headers, stream=True, timeout=10)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Try to get the filename from the Content-Disposition header\n",
        "        content_disposition = response.headers.get('Content-Disposition')\n",
        "        if content_disposition:\n",
        "            filename = content_disposition.split('filename=')[1].strip('\"')\n",
        "        else:\n",
        "            filename = url.split('/')[-1]\n",
        "\n",
        "        if not filename.lower().endswith('.pdf'):\n",
        "            filename += '.pdf'\n",
        "\n",
        "        filepath = os.path.join(folder, filename)\n",
        "        with open(filepath, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        logging.info(f\"Downloaded: {filepath}\")\n",
        "        return True\n",
        "    except requests.RequestException as e:\n",
        "        logging.error(f\"Failed to download {url}: {str(e)}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Unexpected error downloading {url}: {str(e)}\")\n",
        "    return False\n",
        "\n",
        "def main():\n",
        "    keywords = input(\"Enter search keywords for your research: \")\n",
        "    num_results = int(input(\"Enter the number of PDFs to search for: \"))\n",
        "    download_folder = \"research_pdfs\"\n",
        "\n",
        "    if not os.path.exists(download_folder):\n",
        "        os.makedirs(download_folder)\n",
        "\n",
        "    pdf_urls = search_google_scholar(keywords, num_results)\n",
        "\n",
        "    if not pdf_urls:\n",
        "        print(f\"No PDF results found for the query: {keywords}\")\n",
        "        return\n",
        "\n",
        "    successful_downloads = 0\n",
        "    for url in pdf_urls:\n",
        "        if download_pdf(url, download_folder):\n",
        "            successful_downloads += 1\n",
        "        time.sleep(2 + random.random() * 3)  # Be polite, wait between downloads\n",
        "\n",
        "    print(f\"\\nDownload complete. {successful_downloads} out of {len(pdf_urls)} PDFs downloaded to {download_folder}\")\n",
        "\n",
        "    if successful_downloads == 0:\n",
        "        print(\"Troubleshooting tips:\")\n",
        "        print(\"1. Check your internet connection.\")\n",
        "        print(\"2. Try different keywords or increase the number of results.\")\n",
        "        print(\"3. Some PDFs might be inaccessible due to permissions or paywalls.\")\n",
        "        print(\"4. Check the log for more detailed error messages.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32aX4DrBuLDd",
        "outputId": "bfd3ed25-a3eb-4621-f676-774fb1b15e16"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter search keywords for your research: gilgit\n",
            "Enter the number of PDFs to search for: 5\n",
            "\n",
            "Download complete. 5 out of 5 PDFs downloaded to research_pdfs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "al2mOpgwuaeK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}